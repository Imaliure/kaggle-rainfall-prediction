# ğŸŒ§ï¸ Rainfall Prediction with Stacked Models

This project was developed for the **Kaggle Rainfall Prediction Competition**.  
A **stacking ensemble** of LightGBM, XGBoost, and Random Forest with a Logistic Regression meta-model was implemented to achieve high performance.  

ğŸ“Œ Currently ranked **2nd place ğŸ¥ˆ** on the leaderboard ğŸš€  

---

## ğŸš€ Features
- âœ… **Feature Engineering**: outlier clipping, feature removal, interaction features
- âœ… **Base models**: LightGBM, XGBoost, Random Forest
- âœ… **Stacking approach** with Logistic Regression meta-model
- âœ… **Cross Validation with OOF predictions** for reliable evaluation
- âœ… ROC AUC and ROC Curve for performance analysis
- âœ… Automatic Kaggle-ready `submission.csv` generation

---

## ğŸ“Š Dataset
- **train.csv**: Training data
- **test.csv**: Kaggle test set  
- Target variable: `rainfall` (0 = no rain, 1 = rain)

Feature engineering highlights:
- Dropped less useful variables (`mintemp`, `maxtemp`, `temparature`, `winddirection`)
- Kept more predictive features (`pressure`, `dewpoint`, `humidity`, `cloud`, `sunshine`, `windspeed`)

---

## âš™ï¸ Tech Stack
- Python 3.10
- Pandas, NumPy
- Scikit-learn
- LightGBM
- XGBoost
- Matplotlib, Seaborn

---

## ğŸ“ˆ Model Performance

- **Base Models AUC:**
  - LightGBM: ~0.86
  - XGBoost: ~0.85
  - RandomForest: ~0.83
- **Stacked Meta Model AUC:** ~0.88 ğŸ¯

ROC Curve:  
![alt text](roc-curve.png)

---

## ğŸ† Kaggle Leaderboard
My submission result:  
![alt text](kaggle_leaderboard.png)

---

## ğŸ”§ Usage

```bash
# 1) Create an enviroment
python -m venv venv

# 2) Install dependencies
pip install -r requirements.txt

# 3) Place dataset files in the root directory
train.csv
test.csv

# 4) Run the training & inference script
python main.py

# 5) Get Kaggle-ready submission file
submission.csv
